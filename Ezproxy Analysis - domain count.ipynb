{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Add one month's worth of daily log files to the same directory that has this file in it. In the same directory, create a folder called 'csv'. \n",
    "Step 2: Run the code in the next box - click in box and press Shift+Enter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to count domain hits\n",
    "def domain_count(filename):\n",
    "    \n",
    "    import csv\n",
    "    import re\n",
    "    # create csv file from log file\n",
    "    with open(filename,'r') as fh:\n",
    "        with open('csv/' + filename + '.csv','w') as outfile:\n",
    "            for line in fh:\n",
    "                print(re.sub(r'\\n|\"','',line), file=outfile)\n",
    "    import pandas as pd\n",
    "    from urllib.parse import unquote\n",
    "    # create dataframe from csv file skipping malformed lines\n",
    "    df = pd.read_csv('csv/' + filename + '.csv',sep=' ', error_bad_lines=False, header=None, encoding='utf-8')\n",
    "    # remove unnecessary columns\n",
    "    df.drop(df.columns[[4, 5, 7, 8]], axis=1, inplace=True)\n",
    "    # name columns\n",
    "    df.columns = ['ip', 'session_id', 'user_id', 'date_time', 'url', 'size']\n",
    "    # formate date/time column\n",
    "    df['date_time'] = df['date_time'].map(lambda x: x.lstrip('['))\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format='%d/%b/%Y:%H:%M:%S')\n",
    "    # remove lines where user is not logged in\n",
    "    df = df[df.user_id != \"-\"]\n",
    "    # decode urls\n",
    "    def decode_url(url):\n",
    "        decoded_url = unquote(url)\n",
    "        return decoded_url\n",
    "    df['url'] = df.url.apply(decode_url)\n",
    "    # remove excess columns for domain\n",
    "    df.drop(['ip','session_id','size'], axis=1, inplace=True)\n",
    "    # remove ezp string from start of url\n",
    "    df['url'] = df['url'].str.replace(r'^https://ezp\\.lib\\.unimelb\\.edu\\.au:443/login\\?url=|^http://ezp\\.lib\\.unimelb\\.edu\\.au:443/login\\?url=', '')\n",
    "    # remove http etc\n",
    "    df['url'] = df['url'].str.replace(r'^http://www\\.|^https://www\\.|^http://|^https://', '')\n",
    "    # remove ezproxy string from start of url\n",
    "    def parse_url(url):\n",
    "        if (url.startswith(\"ezp.lib.unimelb.edu.au:443\")) and (\"http\" in url):\n",
    "            location = url.find(\"http\")\n",
    "            return url[location:]\n",
    "        elif (url.startswith(\"ezp.lib.unimelb.edu.au:443\")):\n",
    "            return \"-\"\n",
    "        else:\n",
    "            return url\n",
    "    df['url'] = df.url.apply(parse_url)\n",
    "    # remove http etc\n",
    "    df['url'] = df['url'].str.replace(r'^http://www\\.|^https://www\\.|^http://|^https://', '')\n",
    "    # remove rows where ezproxy string is the only url\n",
    "    df = df[df.url != \"-\"]\n",
    "    # remove spaces introduced by unquoting\n",
    "    df['url'] = df['url'].str.replace(r'\\n', '')\n",
    "    # remove everything after : or / or ?\n",
    "    df['url'] = df['url'].str.replace(r'[:/?].*$', '')\n",
    "    # remove .ezp.lib.unimelb.edu.au from urls\n",
    "    df['url'] = df['url'].str.replace(r'\\.ezp\\.lib\\.unimelb\\.edu\\.au', '')\n",
    "    df['url'] = df['url'].str.replace(r'ezp\\.lib\\.unimelb\\.edu\\.au', '-')\n",
    "    df = df[df.url != \"-\"]\n",
    "    # create new column of domains\n",
    "    def get_domain(url):\n",
    "        regexp = re.compile(r'\\.com|\\.org|\\.net|\\.edu|-org|-com|\\.gov')\n",
    "        if regexp.search(url) is not None:\n",
    "            for match in regexp.finditer(url):\n",
    "                location = match.start()\n",
    "            new_url = url[:location]\n",
    "            if ('.' in new_url):\n",
    "                location = new_url.rfind('.')\n",
    "            elif ('-' in new_url):\n",
    "                location = new_url.rfind('-')\n",
    "            else:\n",
    "                return url\n",
    "            location += 1\n",
    "            \n",
    "            if (\"-org\" in url[location:]):\n",
    "                modified_url = url[location:].replace(r'-org', '.org')\n",
    "            elif (\"-com\" in url[location:]):\n",
    "                modified_url = url[location:].replace(r'-com', '.com')\n",
    "            else:\n",
    "                return url[location:]\n",
    "            return modified_url\n",
    "        else:\n",
    "            return url\n",
    "    df['domain'] = df.url.apply(get_domain)\n",
    "    # remove duplicate rows which have same user_id, date-time and domain. \n",
    "    df.drop_duplicates(subset=['user_id', 'date_time','domain'], inplace=True)\n",
    "    df_domains = df['domain'].value_counts().reset_index()\n",
    "    # rename columns\n",
    "    df_domains.columns = ['domain', 'count']\n",
    "    # create csv file from daily domain count dataframe\n",
    "    df_domains.to_csv('daily_domains.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Run the code in the following box. This will run for up to an hour. \n",
    "Wait until the circle in the top right corner is no longer full before moving on to next step.\n",
    "Step 4: Check that the domains.csv file looks ok. It should have 2 columns: domain and count.\n",
    "Rename it to something like this: '2015_07_ezproxy_database_usage.csv'\n",
    "Delete daily_domains.csv\n",
    "Delete all the csv files in the csv directory.\n",
    "Delete all of the log files that you have just analysed and copy and paste in the next month's worth of log\n",
    "files to be analysed.\n",
    "Step 5: Repeat steps 3 and 4 until you have analysed all of the months that you want to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 357171: expected 10 fields, saw 21\\nSkipping line 357185: expected 10 fields, saw 21\\n'\n",
      "b'Skipping line 740449: expected 10 fields, saw 29\\n'\n",
      "b'Skipping line 3942: expected 10 fields, saw 21\\nSkipping line 5176: expected 10 fields, saw 21\\n'\n",
      "b'Skipping line 784107: expected 10 fields, saw 17\\n'\n",
      "b'Skipping line 831933: expected 10 fields, saw 21\\nSkipping line 844347: expected 10 fields, saw 21\\nSkipping line 844397: expected 10 fields, saw 21\\n'\n",
      "b'Skipping line 927104: expected 10 fields, saw 21\\nSkipping line 928974: expected 10 fields, saw 21\\nSkipping line 936222: expected 10 fields, saw 21\\n'\n",
      "b'Skipping line 669858: expected 10 fields, saw 21\\n'\n",
      "b'Skipping line 725957: expected 10 fields, saw 11\\nSkipping line 753551: expected 10 fields, saw 19\\nSkipping line 753626: expected 10 fields, saw 18\\nSkipping line 753635: expected 10 fields, saw 18\\n'\n",
      "b'Skipping line 789735: expected 10 fields, saw 21\\n'\n",
      "b'Skipping line 930162: expected 10 fields, saw 21\\n'\n",
      "b'Skipping line 1227: expected 10 fields, saw 21\\n'\n",
      "b'Skipping line 585694: expected 10 fields, saw 23\\nSkipping line 585695: expected 10 fields, saw 23\\n'\n",
      "b'Skipping line 1024557: expected 10 fields, saw 14\\nSkipping line 1040833: expected 10 fields, saw 17\\nSkipping line 1040939: expected 10 fields, saw 15\\n'\n",
      "b'Skipping line 130541: expected 10 fields, saw 18\\n'\n",
      "b'Skipping line 152811: expected 10 fields, saw 24\\n'\n",
      "b'Skipping line 325440: expected 10 fields, saw 11\\nSkipping line 325458: expected 10 fields, saw 11\\n'\n",
      "b'Skipping line 469509: expected 10 fields, saw 20\\nSkipping line 469584: expected 10 fields, saw 21\\n'\n",
      "b'Skipping line 145071: expected 10 fields, saw 11\\nSkipping line 145075: expected 10 fields, saw 11\\nSkipping line 145076: expected 10 fields, saw 11\\n'\n",
      "b'Skipping line 27225: expected 10 fields, saw 28\\nSkipping line 28096: expected 10 fields, saw 19\\nSkipping line 28155: expected 10 fields, saw 25\\nSkipping line 28162: expected 10 fields, saw 25\\nSkipping line 28167: expected 10 fields, saw 25\\nSkipping line 28169: expected 10 fields, saw 25\\nSkipping line 28176: expected 10 fields, saw 25\\nSkipping line 28179: expected 10 fields, saw 25\\nSkipping line 28185: expected 10 fields, saw 25\\nSkipping line 28190: expected 10 fields, saw 25\\nSkipping line 28193: expected 10 fields, saw 25\\nSkipping line 28194: expected 10 fields, saw 25\\n'\n",
      "b'Skipping line 429499: expected 10 fields, saw 24\\nSkipping line 429506: expected 10 fields, saw 25\\n'\n",
      "b'Skipping line 227478: expected 10 fields, saw 12\\n'\n",
      "b'Skipping line 393021: expected 10 fields, saw 14\\n'\n",
      "b'Skipping line 406768: expected 10 fields, saw 12\\nSkipping line 407519: expected 10 fields, saw 16\\n'\n"
     ]
    }
   ],
   "source": [
    "# main code to process analyse daily log files and create domain count csv file\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "# loop through files in current directory which begin with 'ezproxy.log.*'\n",
    "# modify this if your log file name starts with something else\n",
    "for filename in glob.iglob('ezproxy.log.*'):\n",
    "    # call log analysis function \n",
    "    domain_count(filename)\n",
    "    # create dataframe from csv output file from daily domain count\n",
    "    df_daily = pd.read_csv('daily_domains.csv',sep=',', encoding='utf-8')\n",
    "    import os.path\n",
    "    # if domains.csv already exists - add daily domain counts to domains.csv\n",
    "    if os.path.isfile('domains.csv'):\n",
    "        # create dataframe from domains.csv\n",
    "        df = pd.read_csv('domains.csv',sep=',')\n",
    "        # rename daily dataframe count column to 'daily_count'\n",
    "        df_daily.rename(columns={'count': 'daily_count'}, inplace=True)\n",
    "        # merge the daily and accumulative dataframes\n",
    "        df_merge = pd.merge(df, df_daily, on='domain', how='outer')\n",
    "        # replace all NaN values with 0\n",
    "        df_merge.fillna(0, inplace=True)\n",
    "        # add new column with total of the two count columns\n",
    "        df_merge['total_count'] = df_merge['count'] + df_merge['daily_count']\n",
    "        # drop first 2 columns\n",
    "        df_merge.drop(df_merge.columns[[1,2]], axis=1, inplace=True)\n",
    "        # rename count column\n",
    "        df_merge.rename(columns={'total_count': 'count'}, inplace=True)\n",
    "        # create csv file from resulting dataframe\n",
    "        df_merge.to_csv('domains.csv',index=False, encoding='utf-8')\n",
    "    # first time through, convert domain count dataframe results to csv file\n",
    "    else:\n",
    "        df_daily.to_csv('domains.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: For each monthly file you created in the above process, you need to run the code in the next box. \n",
    "Each time you run it, you will need to modify the filename to the file that you are running.\n",
    "So this code starts with June 2016 - '2016_06_ezproxy_database_usage.csv'. The first time you run the code,\n",
    "you also need to change the column name in the 'else' section to the month name, in this case it's 'Jun-16'. \n",
    "For subsequent times you run it, change the month column name in the 'if' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combines monthly files into one spreadsheet\n",
    "import csv\n",
    "import pandas as pd\n",
    "# create dataframe from monthly domain count csv file\n",
    "# each time you run this code, change the file name as you work through each month\n",
    "# creates dataframe from monthly totals csv file\n",
    "df_one = pd.read_csv('2015_07_ezproxy_database_usage.csv',sep=',', encoding='utf-8')\n",
    "import os.path\n",
    "# second or more run through\n",
    "# merge monthly file with totals file\n",
    "if os.path.isfile('2015-2016_ezproxy_usage_monthly_totals.csv'):\n",
    "    # create dataframe from existing totals csv file\n",
    "    df_two = pd.read_csv('2015-2016_ezproxy_usage_monthly_totals.csv',sep=',', encoding='utf-8')\n",
    "    #drop rows with domain = 0\n",
    "    df_two = df_two[df_two.domain != \"0\"]\n",
    "    # merge two dataframes\n",
    "    df_merge = pd.merge(df_one, df_two, on='domain', how='outer')\n",
    "    # remove NaN values with 0\n",
    "    df_merge.fillna(0, inplace=True)\n",
    "    # rename count column to month/year\n",
    "    # change 'Jul-15' to whatever month file you are running\n",
    "    df_merge.rename(columns={'count': 'Jul-15'}, inplace=True)\n",
    "    # create csv from resulting dataframe\n",
    "    df_merge.to_csv('2015-2016_ezproxy_usage_monthly_totals.csv',index=False, encoding='utf-8')\n",
    "# first run through\n",
    "else:\n",
    "    # rename count of first monthly count column, so replace 'Jun-16' with the month you are starting with\n",
    "    df_one.rename(columns={'count': 'Jun-16'}, inplace=True)\n",
    "    #drop rows with domain = 0\n",
    "    df_one = df_one[df_one.domain != \"0\"]\n",
    "    # convert dataframe to csv\n",
    "    df_one.to_csv('2015-2016_ezproxy_usage_monthly_totals.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Depending on how many months and which months you are processing, rename columns in the code below.\n",
    "Run the code below and your csv spreadsheet is complete. You can then open it in Excel, make it prettier and\n",
    "save it as a .xlsx file if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 6-month and yearly total columns\n",
    "import csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('2015-2016_ezproxy_usage_monthly_totals.csv',sep=',', encoding='utf-8')\n",
    "# create new columns of 6-monthly and yearly totals\n",
    "df['Jul-Dec_15_Total'] = df['Jul-15'] + df['Aug-15'] + df['Sep-15'] + df['Oct-15'] + df['Nov-15'] + df['Dec-15']\n",
    "df['Jan-Jun_16_Total'] = df['Jan-16'] + df['Feb-16'] + df['Mar-16'] + df['Apr-16'] + df['May-16'] + df['Jun-16']\n",
    "df['Yearly_Total'] = df['Jul-Dec_15_Total'] + df['Jan-Jun_16_Total']\n",
    "#sort on Yearly_Total\n",
    "df.sort_values(by='Yearly_Total', ascending=0, inplace=True)\n",
    "# create csv file from dataframe\n",
    "df.to_csv('2015-2016_ezproxy_usage_monthly_totals.csv',index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
